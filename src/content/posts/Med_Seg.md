---
title: å‡ ç¯‡å…³äºåŒ»å­¦å›¾åƒæ–¹é¢çš„è®ºæ–‡è§£è¯»
published: 2025-10-01
description: 'è®ºæ–‡è§£è¯»'
image: ''
tags: [åŒ»å­¦å›¾åƒ]
category: 'æ·±åº¦å­¦ä¹ '
draft: false 
lang: 'zh_CN'
---

## ShaSpec- the first missing modality multi-modal approach 

### ä¸€äº›è¦çŸ¥é“çš„å†…å®¹

ä»€ä¹ˆæ˜¯æ¨¡æ€ï¼Ÿ

ä¿¡æ¯è¡¨è¾¾çš„å½¢å¼ï¼Œæ¯”å¦‚ç”¨æ–‡æœ¬æˆ–è€…è§†é¢‘å›¾ç‰‡ä¹‹ç±»çš„è¡¨è¾¾æŸä¸ªä¿¡æ¯



ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€ï¼Ÿ

å¤šæ¨¡æ€æŒ‡çš„æ˜¯æ•°æ®æˆ–è€…ä¿¡æ¯çš„**å¤šç§**è¡¨ç°å½¢å¼





### Abstract

1. å½“å‰çš„æ–¹æ³•åœ¨evaluationæˆ–è€…train separate modelå»å¤„ç†ç‰¹å®šçš„æ¨¡æ€ç¼ºå¤±
2. these modelsï¼ˆæŒ‡çš„æ˜¯å“ªäº›ï¼Ÿï¼‰è‡´åŠ›äºå¤„ç†æŸä¸ªç‰¹å®šçš„ä»»åŠ¡(ä»ä¸‹æ–‡å¯ä»¥çœ‹åˆ°è¿™ä¸ªæ–¹æ³•åœ¨åˆ†å‰²/ åˆ†ç±»ä¸Šé¢å¤„ç†çš„éƒ½æŒºä¸é”™çš„)

***Sha***red-***Spec***ific Feature Modelling å…±äº«ç‰¹å®šç‰¹å¾å»ºæ¨¡

#### å¦‚ä½•åšåˆ°ï¼Ÿ

1. ShaSpec is designed to take advantage of all available input modalities during training and evaluation by learning shared and specific features to better represent the input data.  ShaSpecæ—¨åœ¨é€šè¿‡***å…±äº«å­¦ä¹ ***ï¼ˆå…±äº«å‚æ•°,å…±äº«åŒä¸€ä¸ªæ¨¡å‹ï¼‰å’Œ***å­¦ä¹ ç‰¹å®šç‰¹å¾***æ¥æ›´å¥½åœ°è¡¨ç¤ºè¾“å…¥æ•°æ®ï¼Œä»è€Œåœ¨è®­ç»ƒå’Œè¯„ä¼°æœŸé—´åˆ©ç”¨æ‰€æœ‰å¯ç”¨çš„è¾“å…¥æ¨¡æ€ã€‚
2. This is achieved from a strategy that relies on auxiliary tasks based on distribution alignment and domain classification, in addition to a residual feature fusion procedure. é€šè¿‡ä¾èµ–äºåŸºäºåˆ†å¸ƒå¯¹é½å’ŒåŸŸåˆ†ç±»çš„è¾…åŠ©ä»»åŠ¡ä»¥åŠå‰©ä½™ç‰¹å¾èåˆè¿‡ç¨‹çš„ç­–ç•¥æ¥å®ç°çš„ã€‚ï¼ˆè¿™é‡Œåé¢å¯ä»¥çœ‹åˆ°æ˜¯é€šè¿‡æ·»åŠ äº†ä¸¤ç§æ–°çš„æŸå¤±ç­–ç•¥æ¥è¾¾æˆçš„ï¼‰
3. The design simplicity of ShaSpec enables its easy adaptation to multiple tasks, such as classification and segmentation. ShaSpecçš„è®¾è®¡ç®€å•æ€§ï¼ˆå¤§é“è‡³ç®€ï¼‰ä½¿å…¶æ˜“äºé€‚åº”å¤šä¸ªä»»åŠ¡ï¼Œä¾‹å¦‚åˆ†ç±»å’Œåˆ†æ®µ





![image-20250928233719430](imgs/med_seg/1-1.png)



![image-20250928231339150](imgs/med_seg/1-2.png)

#### note

è¿™ä¸¤å¼ å›¾çš„Decoderä»…ä»…ç”¨äºsegmentation

å¦‚æœè¦ç”¨äºclassificationï¼Œèåˆçš„ç‰¹å¾å°†è¢«å–‚ç»™FCå±‚ï¼Œ



#### ä½œè€…å¯¹è¿™äº›æ¨¡å—ä½œç”¨çš„è¯´æ˜

$$
s^{\{i\}}è¡¨ç¤ºçš„æ˜¯æ¨¡æ€é—´çš„å¼‚æ„æ€§ï¼Œr^{\{i\}}æ•æ‰ç‰¹å¾é—´çš„ä¸€è‡´æ€§
$$

#### ç¼ºå¤±æ¨¡æ€çš„è¯´æ˜

å…¶ä»–åœ°æ–¹æ˜¯ä¸€æ ·çš„ï¼Œåªæœ‰å¯¹ç¡®å®æ¨¡æ€ä¸­çš„fæ˜¯ç›´æ¥ç”Ÿæˆçš„


$$
å‡å®šnæ˜¯ç¼ºå¤±çš„æ¨¡æ€ï¼Œf^{(n)}=\frac{1}{N-1}\sum_{i=1,iâ‰ n}^Nr^{\{i\}}
$$
![image-20250929001546869](imgs/med_seg/1-3.png)





è¿™æ®µè¯æˆ‘æ²¡ææ‡‚ï¼Œå¦‚æœæœ‰â‰¥2çš„æ¨¡æ€ç¼ºå¤±çš„è¯ï¼Œé‚£åº”è¯¥å¦‚ä½•ç”Ÿæˆï¼Œå…¬å¼4ä¸åº”è¯¥åªç»™å‡ºäº†åªç¼ºå°‘å…¶ä¸­ä¸€ç§æ¨¡æ€çš„æƒ…å†µå—ï¼Ÿ

æ¨å¹¿åˆ°å¤šä¸ªæ¨¡æ€ç¼ºå¤±çš„å…¬å¼ä¸ºï¼š
$$
f^{(n)}=\frac{1}{å¯ç”¨æ¨¡æ€æ•°é‡}\sum_{i=1,iâ‰ n}^Nr^{\{i\}}
$$
å°±æ˜¯è¯´å…¶ä»–çš„ç¼ºå¤±æ¨¡æ€éƒ½ä¼šå˜æˆç›¸åŒçš„å€¼ï¼Œæ¯”å¦‚ç¼ºå¤±T1å’ŒT2,ç”¨FLå’ŒT1cçš„å¹³å‡å€¼æ¥è·å–ï¼Œæœ€ç»ˆT1å’ŒT2çš„å€¼ä¼šæ˜¯ç›¸åŒçš„











### è®­ç»ƒè¿‡ç¨‹

besides optimising for the main task (segmentation or classification), we introduce two auxiliary tasks, domain classification and distribution alignment, for the learning of the specific and shared feature representations, respectively. é™¤äº†å¯¹ä¸»è¦ä»»åŠ¡ï¼ˆåˆ†å‰²æˆ–åˆ†ç±»ï¼‰è¿›è¡Œä¼˜åŒ–ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼ŒåŸŸåˆ†ç±»å’Œåˆ†å¸ƒå¯¹é½ï¼Œåˆ†åˆ«ç”¨äºå­¦ä¹ ç‰¹å®šå’Œå…±äº«ç‰¹å¾è¡¨ç¤ºã€‚



æ–‡ä¸­å¹¶æ²¡æœ‰ä»‹ç»åˆ†å‰²å’Œåˆ†ç±»çš„è®­ç»ƒï¼Œè€Œæ˜¯ä»‹ç»äº†â€œDomain Classification Objectiveâ€å’Œâ€œ Distribution Alignment Objectiveâ€ä»¥åŠâ€œOverall Objectiveâ€



#### Domain Classification Objective

we propose to adopt the domain classification objective (DCO) for the specific feature learning. æå‡ºè¿™ä¸ªé˜ˆåˆ†ç±»ç›®æ ‡ç”¨äºç‰¹å®šç‰¹å¾å­¦ä¹ 


$$
L_{dco}(\mathcal{D},\theta^{spec},\theta^{dco})=-\sum_{j=1}^{|\mathcal{D}|}\sum_{i=1}^N(t^{(i)})^{\top}log(f_{\theta^{dco}}(s_j^{(i)}))\\
t^{(i)}\in\{0,1\}^N,å…¶ä¸­1æ˜¯ç¬¬iä¸ªä½ç½®ï¼Œå…¶ä»–éƒ½æ˜¯0ï¼Œæ¯”å¦‚è¯´å¯¹äºFlairæ¨¡æ€ï¼Œå®ƒçš„æ ‡ç­¾æ˜¯ [1, 0, 0, 0]ï¼›å¯¹äºT1æ¨¡æ€ï¼Œæ˜¯ [0, 1, 0, 0]ï¼Œä»¥æ­¤ç±»æ¨ã€‚\\
s^{(i)}=f_{\theta^{spec}}^{(i)}(x^{(i)})\\
$$


ç”¨äºè®¡ç®—ç‰¹å®šç¼–ç å™¨çš„æŸå¤±å€¼ï¼Œå…¶å®æ„Ÿè§‰å°±æ˜¯ç”¨äºä¼˜åŒ–Specific Encoder

Nï¼šæ¨¡æ€æ€»æ•°ï¼ˆ4ç§MRIæ¨¡æ€ï¼šFlair, T1, T1c, T2ï¼‰

Dï¼šè®­ç»ƒæ•°æ®é›†
$$
\mathcal{D}=\{(\mathcal{M}_j,y_j)\}_{j=1}^{|\mathcal{D}|}\\
\mathcal{M}_j:ç¬¬jä¸ªè®­ç»ƒæ ·æœ¬çš„å¤šæ¨¡æ€æ•°æ®é›†åˆï¼Œå…¶å®å°±ç±»ä¼¼äºä½ ç°æœ‰çš„æ•°æ®æ˜¯å¤šå°‘ï¼Œæ¢ä¸ªè¯´æ³•å°±æ˜¯x
$$






#### Distribution Alignment Objective

$$
L_{dao}(\mathcal{D},\theta^{sha},\theta^{dco})=-\sum_{j=1}^{|\mathcal{D}|}\sum_{i=1}^N(u^{(i)})^{\top}log(f_{\theta^{dao}}(r_j^{(i)}))\\
r^{(i)}=f_{\theta^{sha}}(x^{(i)})
$$



è¿™éƒ¨åˆ†å°±æ˜¯ç”¨äºä¼˜åŒ–Shared Encoderï¼Œ å¯ä»¥ä»å…¬å¼7å’Œ8çœ‹åˆ°ï¼Œè¿™éƒ¨åˆ†å…¶å®å°±æ˜¯è®¡ç®—

![image-20250929225446870](imgs/med_seg/1-10.png)

![image-20250929225455708](imgs/med_seg/1-11.png)





#### Overall Objective*ä¸»è¦


$$
L_{total}(\mathcal{D},\theta^{sha},\theta^{spec},\theta^{proj},\theta^{dao},\theta^{dco},\theta^{dec})=L_{task}(\mathcal{D},\theta^{sha},\theta^{spec},\theta^{proj},\theta^{dec})+\alpha L_{dao}(\mathcal{D},\theta^{sha},\theta^{dao})+\beta L_{dco}(\mathcal{D},\theta^{spec},\theta^{dco})
$$






#### æ•°æ®é›†ï¼š

a. **BraTS2018 for medical image segmentation** 

1.  The BraTS2018 Segmentation Challenge dataset [1,21] is used as a multi-modal learning with missing modality brain tumour sub-region segmentation benchmark,  ä»¥ä¸‹ä¸‰ä¸ªåˆ†å‰²å­åŒºåŸŸ
    1.  where the sub-regions are **enhancing tumour (ET)**
    2.  **tumour core (TC)**
    3.  **whole tumour (WT)**
2.  BraTS2018 contains 3D multi-modal brain MRIs, æ•°æ®é›†åŒ…å«ä»¥ä¸‹å››ç§æ¨¡æ€
    1.  including **Flair,** 
    2.  **T1,** 
    3.  **T1 contrast-enhanced (T1c)** 
    4.  **T2** with experienced imaging experts annotated ground-truth. 



| å‚æ•°                                                         | å…·ä½“å†…å®¹                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| stochastic gradient descent optimizer                        | Nesterov momentum of 0.99                                    |
| backbone network                                             | **3D UNet**(where the<br/>fusion of shared and specific features happens at the bottom<br/>of the UNet structure.) |
| learning rate                                                | 0.01 at the beginning and decreased with cosine annealing strategy  ä½™å¼¦é€€ç«ç­–ç•¥ |
| During the non-dedicated training of models, modalities are<br/>randomly dropped to simulate the modality-missing situations. | åœ¨æ¨¡å‹çš„éä¸“ç”¨è®­ç»ƒæœŸé—´ï¼Œæ¨¡æ€è¢«éšæœºä¸¢å¼ƒä»¥æ¨¡æ‹Ÿæ¨¡æ€ç¼ºå¤±çš„æƒ…å†µã€‚ |
| For dedicated training of models, the missing modalities used for training are the same missing modalities in<br/>the evaluation. | å¯¹äºæ¨¡å‹çš„ä¸“ç”¨è®­ç»ƒï¼Œç”¨äºè®­ç»ƒçš„ç¼ºå¤±æ¨¡æ€ä¸evalä¸­çš„ç¼ºå¤±æ¨¡æ€ç›¸åŒã€‚ |
| iterations                                                   | 180,000                                                      |
| distribution alignment objective loss function               | L1 loss                                                      |
| Î± = 0.1, Î² = 0.02                                            |                                                              |
| ShaSpec*                                                     | prediction smoothness enhancement é¢„æµ‹å¹³æ»‘å¢å¼º               |

![image-20250929142014396](imgs/med_seg/1-4.png)





è¡¨ä¸€æ˜¯éä¸“ç”¨è®­ç»ƒçš„æ¨¡å‹ï¼ˆä¸€ä¸ªæ¨¡å‹åº”å¯¹æ‰€æœ‰çš„æƒ…å†µï¼‰ï¼Œè¡¨2æ˜¯ä¸“ç”¨è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¤šä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹åº”å¯¹ä¸€ç§æƒ…å†µï¼‰

**è¿™ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**æ¨¡å‹æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯**è®­ç»ƒç­–ç•¥**ä¸åŒï¼Œ

**ä½†æ˜¯è¿™æ ·å­ä¸ºä»€ä¹ˆè¡¨ä¸€å’Œè¡¨äºŒåœ¨æ¨¡æ€ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå¯¹æŸä¸ªåŒºåŸŸè¿›è¡Œåˆ†å‰²æœ€ç»ˆè¾“å‡ºçš„ç»“æœä¸åŒï¼Ÿ**è§£é‡Šæ˜¯è¯´äºŒè€…æ‰€æä¾›çš„æ•°æ®å…¶å®æ˜¯ä¸åŒçš„ï¼ˆè®ºæ–‡æ˜¯è¿™æ ·å­è¯´æ˜çš„ï¼šDuring the non-dedicated training of models, modalities are randomly dropped to simulate the modality-missing situations. For dedicated training of models, the missing modalities used for training are the same missing modalities in the evaluation.åœ¨æ¨¡å‹çš„éä¸“ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡æ€è¢«éšæœºä¸¢å¼ƒä»¥æ¨¡æ‹Ÿæ¨¡æ€ç¼ºå¤±çš„æƒ…å†µã€‚å¯¹äºæ¨¡å‹çš„ä¸“ç”¨è®­ç»ƒï¼Œç”¨äºè®­ç»ƒçš„ç¼ºå¤±æ¨¡æ€ä¸è¯„ä¼°ä¸­çš„ç¼ºå¤±æ¨¡æ€ç›¸åŒã€‚ï¼‰<span style="color: red; font-weight: bold;">å°±æ˜¯è¯´éä¸“ç”¨æ¨¡å‹ç”¨æŸä¸ªæ¨¡æ€è®­ç»ƒï¼Œæœ€ç»ˆæµ‹è¯•çš„æ—¶å€™å¯ä»¥ç”¨ä»»æ„ç§æ¨¡æ€ä½œä¸ºæ•°æ®è¾“å…¥ï¼Œè€Œä¸“ç”¨è®­ç»ƒå°±æ˜¯è®­ç»ƒå’Œæµ‹è¯•çš„æ—¶å€™æ•°æ®éƒ½æ˜¯ç›¸åŒçš„</span>







b. **Audiovision-MNIST for computer vision classification**

1. éŸ³é¢‘æ‰‹å†™æ•°å­—é›†
2. a multi-modal dataset consisting of 1500 samples of audio and image files.
3. é‡‡ç”¨å’ŒSMILæ¨¡å‹ä¸€è‡´çš„å‚æ•°
4. ä¸åŒçš„æ˜¯æœ€åçš„Decoderæ˜¯ç”±FC-dropout-FCç»„æˆ
5. æ¨¡å‹è®­ç»ƒä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œæƒé‡è¡°å‡ä¸º10âˆ’2ï¼Œåˆå§‹å­¦ä¹ ç‡è®¾ç½®ä¸º10âˆ’3ï¼ˆæ¯20ä¸ªepochå‡å°‘10%ï¼‰



![image-20250929145301715](imgs/med_seg/1-5.png)

![image-20250929145701180](imgs/med_seg/1-6.png)



#### åšäº†ä¸€äº›å…¶ä»–çš„å®éªŒ

##### **The selection of DAO loss function**

L1ï¼KLï¼MSEï¼CE

![image-20250929145848163](imgs/med_seg/1-7.png)

##### **Sensitivity of Eq. (9) to Î± and Î²:** 

æµ‹è¯•Î± æ—¶å€™ï¼Œ Î²=0.02

æµ‹è¯• Î²æ—¶å€™ï¼ŒÎ± =0.1

Î± å’Œ Î²éƒ½ä¸º1çš„æ—¶å€™ï¼Œæ›²çº¿ä¸‹é™çš„å¾ˆå¿«ï¼Œè¡¨æ˜è¾…åŠ©æŸå¤±ç»™å¤ªé«˜æƒé‡ä¼šå¯¼è‡´main taskæ¢¯åº¦æµå—åˆ°å¹²æ‰°

![image-20250929145939768](imgs/med_seg/1-8.png)

Î± =0.1ï¼ŒÎ²=0.02æ˜¯æœ€ä¼˜è§£



**Small values for the weights of the auxiliary tasks contribute to the whole process, but do not interfere with the main task optimisation.** Interestingly, when Î± = 0 (only specific features are learned), the model can still segment the tumours to some extent by simple concatenation of specific features, which means that the specific features contain rich information. A similar conclusion can be reached when Î² = 0 (only shared features are learned). è¿™è¾¹è¯´è¾…åŠ©åˆ†æ”¯çš„æƒå€¼ç»™çš„å°å¯¹æ•´ä¸ªè®­ç»ƒæœ‰æå‡ï¼Œä½†å¹¶ä¸ä¼šå½±å“ä¸»ä»»åŠ¡çš„ä¼˜åŒ–ï¼Œå½“Î± or Î²=0æ—¶å€™ï¼Œæ¨¡å‹ä»ç„¶å¯ä»¥åˆ†å‰²ç›®æ ‡çš„ç‰¹å®šç‰¹å¾ï¼ˆå…¶å®æˆ‘æ„Ÿè§‰è¦æ˜¯è¾…åŠ©åˆ†æ”¯å¯¹æ•´ä½“çš„åˆ†å‰²éƒ½æœ‰å¤§çš„å½±å“ï¼Œé‚£å¯èƒ½éƒ½ä¸æ˜¯è¾…åŠ©äº†è€Œæ˜¯å¦ä¸€ä¸ªå’Œä¸»æ¨¡å‹åŒç­‰çš„æ¨¡å‹äº†ğŸ˜‚ï¼Œè¿™æ®µè¯å¤šå°‘æ˜¯æœ‰ç‚¹å‡‘å­—æ•°çš„æ„Ÿè§‰äº†ï¼‰



##### Computational comparison

we estimate the average time consumption of **30 iterations on one 3090 GPU** for a fair comparison.

|                                   | ShaSpec                                                      | SMIL                                                         |
| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| training/inference iteration time | takes **0.0257s** for model training iteration and **0.0016s** for model testing | training iterations and testing taking 0.1309s and 0.0019s   |
| GPU memory usage                  | constantly consumes **1421MiB** of GPU memory                | the GPU memory usage started<br/>from 1430MiB, climbed to 24268MiB, and then casted an<br/>â€œout of memoryâ€ error in the end. |
| batch-size                        | 4                                                            | 4                                                            |
| model parameters                  | **0.22M parameters**                                         | 0.33M parameters                                             |



##### An additional classification experiment on X-ray + clinical texts

ï¼ˆä¸ªäººè§‰å¾—å¯ä»¥passè¿™éƒ¨åˆ†ï¼‰å…¶å®å°±æ˜¯åœ¨ä¸€ä¸ªä¸´åºŠæ•°æ®é›†åˆ†ç±»ï¼ˆè§†è§‰-æ–‡æœ¬æ•°æ®é›†ï¼‰ä¸Šé¢åˆåšäº†ä¸€æ¬¡å®éªŒ



##### Shared and Specific Feature Visualisation

![image-20250929151534467](imgs/med_seg/1-9.png)

å…±äº«ç‰¹å¾è¢«èšç±»åœ¨ä¸€èµ·ï¼Œè€Œç‰¹å®šç‰¹å¾è¢«å¾ˆå¥½åœ°åˆ†ç¦»

å…¶å®æˆ‘æ¯”è¾ƒå¥½å¥‡è¿™éƒ¨åˆ†ä½œå›¾ä»£ç æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼ˆæ˜¾ç„¶çœ‹äº†ä»£ç è¿™éƒ¨åˆ†ä¼¼ä¹å¹¶æ²¡æœ‰åœ¨å†…ï¼‰



## Multi-Modal Modality-Masked Diffusion Network for Brain MRI Synthesis With Random Modality Missingï¼ˆM2DNï¼‰ 



### Abstract

1. ç°æœ‰çš„åŒ»å­¦å›¾åƒåˆæˆç”Ÿæˆæ–¹æ³•é€šå¸¸åŸºäº**è·å–æ¨¡æ€**å’Œ**ç¼ºå¤±æ¨¡æ€**ä¹‹é—´çš„è·¨æ¨¡æ€è½¬å˜ï¼ˆå¤§æ¦‚æ˜¯æŒ‡çš„æ˜¯ä»å½“å‰æœ‰çš„æ¨¡æ€ä¸­è½¬å˜åˆ°æ²¡æœ‰çš„æ¨¡æ€ï¼‰
2. è¿™äº›æ–¹æ³•ï¼ˆï¼‰é’ˆå¯¹ç‰¹å®šçš„ç¼ºå¤±æ¨¡æ€å¹¶ä¸”**ä¸€æ¬¡æ€§**å®Œæˆåˆæˆç¼ºå¤±çš„æ¨¡æ€ï¼ˆä¸èƒ½çµæ´»å¤„ç†ä¸åŒçš„ç¼ºå¤±æ¨¡æ€çš„æ•°é‡ï¼Œä¹Ÿä¸èƒ½æœ‰æ•ˆçš„æ„å»ºè·¨æ¨¡æ€ä¹‹é—´çš„æ˜ å°„ï¼ˆæ•ˆç‡ä½ï¼‰ï¼‰

a unified Multi-modal Modality-masked Diffusion Network (M2DN), ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ©è”½æ‰©æ•£ç½‘ç»œï¼ˆâ€tackling multi-modal synthesis from the perspective of â€œprogressive whole-modality inpaintingâ€ä»â€œæ¸è¿›å¼å…¨æ¨¡æ€å›¾åƒä¿®å¤â€çš„è§’åº¦å¤„ç†å¤šæ¨¡æ€åˆæˆï¼Œè€Œä¸æ˜¯â€œè·¨æ¨¡æ€è½¬å˜â€ï¼‰(æŠŠç¼ºå¤±çš„æ¨¡æ€å½“ä½œå™ªå£°ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹é€æ­¥å»å™ªç”Ÿæˆ)

**STEP:** 

1. å°†ä¸¢å¤±çš„æ¨¡æ€è§†ä¸ºéšæœºå™ªå£°ï¼Œå¹¶åœ¨æ¯ä¸ªåå‘æ‰©æ•£æ­¥éª¤ä¸­å°†æ‰€æœ‰æ¨¡æ€è§†ä¸ºä¸€ä¸ªæ•´ä½“
2. å¼•å…¥äº†ä¸€ç§æ¨¡æ€æ©ç æ–¹æ¡ˆï¼Œå°†æ¯ä¸ªä¼ å…¥æ¨¡æ€çš„å¯ç”¨æ€§çŠ¶æ€æ˜¾å¼ç¼–ç åœ¨äºŒè¿›åˆ¶æ©ç ä¸­ï¼Œå¹¶å°†å…¶ç”¨ä½œæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºM2DNåœ¨ä»»æ„ç¼ºå¤±åœºæ™¯ä¸‹çš„åˆæˆæ€§èƒ½ï¼ˆè¿™æ®µè¯æ–‡ä¸­å¦å¤–ä¸€ç§è¯´æ³•ï¼š *ä»¥æ˜¾å¼åœ°æŒ‡å¯¼æ¨¡å‹å¤„ç†ä¸åŒçš„ç¼ºå¤±åœºæ™¯*ï¼‰



### è´¡çŒ®

Different from the existing models, M2DN employs multi-input multi-output (MIMO) framework based on multitask learning, by which the missing and available modalities are jointly exploited, fused, and synthesized **ä¸ç°æœ‰æ¨¡å‹ä¸åŒï¼ŒM2DNé‡‡ç”¨åŸºäºå¤šä»»åŠ¡å­¦ä¹ çš„å¤šè¾“å…¥å¤šè¾“å‡ºï¼ˆMIMOï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¯¥æ¡†æ¶ï¼Œç¼ºå¤±æ¨¡æ€å’Œå¯ç”¨æ¨¡æ€è¢«è”åˆåˆ©ç”¨ã€èåˆå’Œåˆæˆ**

1. æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡æ€æ©è”½æ‰©æ•£ç½‘ç»œï¼Œè¢«ç§°ä¸ºM2DNï¼Œåœ¨å•ä¸ªç½‘ç»œå†…ç”¨äºåˆæˆbrain MRIä¸­çš„éšæœºç¼ºå¤±æ¨¡æ€
2. è¯¥æ–¹æ³•å°†æ‰€æœ‰æ¨¡æ€ä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼Œé€šè¿‡ç¼ºå¤±æ¨¡æ€åˆæˆå’Œå¯ç”¨æ¨¡æ€è‡ªé‡æ„å…±åŒå®Œæˆå¤šæ¨¡æ€åˆæˆï¼Œå¹¶åŸºäºå­¦ä¹ åˆ°çš„å…¬å…±æ½œåœ¨ç©ºé—´ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹ä»é«˜æ–¯å™ªå£°ä¸­é‡ç»˜ç¼ºå¤±çš„æ¨¡æ€
3. å¼•å…¥äº†ä¸€ä¸ªäºŒè¿›åˆ¶æ¨¡æ€æ©ç ï¼Œå®ƒåµŒå…¥äº†æ¯ä¸ªæ¨¡æ€çš„å¯ç”¨æ€§çŠ¶æ€ï¼Œä½œä¸ºæ¯ä¸ªå»å™ªæ­¥éª¤çš„æ¡ä»¶ï¼Œè¿™ç§æ˜ç¡®çš„æ¨¡æ€å¯ç”¨æ€§æ ‡å¿—å¯ä»¥æé«˜åˆæˆæ•ˆç‡ã€‚
4. åœ¨ä¸¤ä¸ªå…¬å¼€çš„å¤šæ¨¡æ€MRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„M2DNä¸ä»…æ”¯æŒåœ¨å•ä¸ªç½‘ç»œä¸­å¯¹ä»»æ„ç¼ºå¤±æ¨¡æ€è¿›è¡Œå¯é çš„åˆæˆï¼Œè€Œä¸”æ˜¾ç€æé«˜äº†åˆæˆæ€§èƒ½ã€‚





### æ–¹æ³•

#### å‰ç½®çŸ¥è¯†



| ç¬¦å·              | å«ä¹‰                                                         |
| ----------------- | ------------------------------------------------------------ |
| M,M^s,M^u         | Mæ˜¯é€šè¿‡ä¸åŒæ‰«æåºåˆ—çš„å¤šæ¨¡æ€è„‘MRIå›¾åƒï¼ŒM^sæ˜¯å¯æä¾›çš„æ¨¡æ€ï¼ŒM^uæ˜¯ç¼ºå¤±çš„æ¨¡æ€ï¼Œå…¶ä¸­M=M^s+M^u |
| \mathcal{N}       | é«˜æ–¯åˆ†å¸ƒå‡½æ•°                                                 |
| x_n               | t=næ—¶å€™çš„å›¾                                                  |
| \alphaå’Œ\beta     | éƒ½æ˜¯äººä¸ºå®šä¹‰çš„å‚æ•°                                           |
| I                 | é«˜æ–¯å™ªå£°                                                     |
| qå’Œp              | qï¼šå‰å‘ä¼ æ’­ï¼Œpï¼šåå‘å»å™ªç”Ÿæˆå›¾åƒ                             |
| \epsilon          | Iï¼Œé«˜æ–¯å™ªå£°                                                  |
| \mu_\theta(x_t,t) | è‡ªå®šä¹‰çš„ä¸€ä¸ªç½‘ç»œï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªæ¥è¿‘å’Œå‰å‘ä¼ æ’­çš„å‡å€¼ç½‘ç»œ       |
|                   |                                                              |

$$
\mathcal{N}(x;\mu,\sigma^2),\mu: å‡å€¼ï¼Œ\sigma^2ï¼šæ–¹å·®\\
q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)\\
å‡å®šï¼š\alpha_t:=1-\beta_t\\
\bar{\alpha_t}:=\prod_{s=1}^t\alpha_s\\
q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha_t}}x_0,(1-\bar{\alpha_t})I)\\
$$

![image-20251001151102725](imgs/med_seg/2-1.png)


$$
å‰å‘ä¼ æ’­çš„é€†å‘ï¼š\\
q(x_{t-1}|x_t,x_0)=\int_{x_0}q(x_{t-1}|x_t,x_0)q(x_0|x_t)d{x_0}=
\mathcal{N}(x_{t-1};\tilde{\mu_t}(x_t,x_0),\tilde{\beta}_tI)\\
\tilde{\mu}_t(x_t,x_0):=\frac{\sqrt{\alpha_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t\\
\tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t\\
ç”±äºå‰å‘è¿‡ç¨‹çš„æ–¹å·®\beta_tç›´æ¥å–å¸¸æ•°ï¼Œå› æ­¤æœ€ååå‘è¿‡ç¨‹ç›´æ¥å’Œå‰å‘ä¼ æ’­çš„\betaè®¾ç½®æˆä¸€ä¸ªå€¼äº†\\
å› ä¸ºè¦ä½¿å¾—q(x_T|x_0)ä¸p(x_T)=\mathcal{N}(x_T;0,I)çš„KLæ•£åº¦è¶Šå°ï¼Œå³è®©ä¸¤è€…ç›¸ç­‰å°±æœ‰x_0=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar\alpha}_t}\\
æœ€åé‡‡æ ·çš„ç›®çš„å°±æ˜¯è®­ç»ƒä¸€ä¸ªç½‘ç»œ\mu_\theta(x_t,t)ï¼ˆè‡ªå®šä¹‰çš„ä¸€ä¸ªç½‘ç»œï¼‰ä½¿å…¶ä¸\tilde{\mu}_t(x_t,x_0)çš„è·ç¦»éå¸¸å°ï¼Œå°±æœ‰L_t=E_q[\frac{1}{2\sigma^2_t}||\tilde\mu(x_t,x_0)-\mu_\theta(x_t,t)||^2]+C\\
å°†x_0å’Œ\tilde{\mu}_t(x_t,x_0)ä»£å…¥ï¼Œå°±æœ‰\mu_\theta(x_t,t)=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha_t}}}\epsilon_\theta(x_t,t))+\sigma_tI=\tilde\mu(x_t,\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar\alpha}_t}),å‘ç°é™¤äº†\epsilonä¹‹å¤–å…¶ä»–éƒ½æ˜¯æ—¢å®šçš„å‚æ•°\\
ä¹Ÿå°±æ˜¯è¯´æœ€ç»ˆçš„è®­ç»ƒç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªç½‘ç»œ\epsilon_\thetaä½¿å…¶é¢„æµ‹å‰å‘ä¼ æ’­æ‰€åŠ å™ªçš„å™ªå£°\\
å³æœ€ç»ˆçš„è®­ç»ƒä¸ºï¼šL(\theta):=E_{t,x_0,\epsilon}[||\epsilon-\epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,t)||^2]\\
å³æœ€ç»ˆçš„é‡‡æ ·ä¸ºï¼šx_{t-1}=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha_t}}}\epsilon_\theta(x_t,t))+\sigma_tI
$$
![image-20251001154544547](imgs/med_seg/2-2.png)

![image-20251001154422320](imgs/med_seg/2-3.png)



### ç½‘ç»œç»“æ„

![image-20251001195226226](imgs/med_seg/2-4.png)

1. ç”¨é«˜æ–¯å™ªå£°åˆå§‹åŒ–ç¼ºå¤±çš„æ¨¡æ€
2. å°†ç¼ºå¤±æ¨¡æ€ç¼–ç ä¸º1ï¼Œå¯ç”¨æ¨¡æ€ç¼–ç ä¸º0ï¼ˆmodality-mask schemeæ¨¡æ€æ©ç æ–¹æ¡ˆï¼šä¸ºäº†æ›´å¥½çš„å¤„ç†ä»»æ„æ¨¡æ€ç¼ºå¤±çš„åœºæ™¯ï¼ŒåŒæ—¶ä½œä¸ºæ¡ä»¶æå¤§çš„ä¼ é€’äº†å¯ç”¨æ€§çŠ¶æ€ç»™diffusion-base M2DNï¼‰

![image-20251001200012162](imgs/med_seg/2-5.png)

1. æ¯ä¸ªæ¨¡æ€é€šè¿‡å•ç‹¬çš„ç¼–ç å™¨æå–ç‰¹å¾ï¼Œç„¶åè¿æ¥å¹¶ä¸”è½¬åˆ°å¤šæ¨¡æ€èåˆæ¨¡å—ä¸­ï¼ˆwhere the multi-modal information is exchanged and integrated.ï¼‰
2. â€œ3 Ã— 3 Convï¼ˆstride of 2ï¼‰+GroupNorm+ReLUâ€ 
3. Encoderï¼š  â€œResBlock+bilinear downsamplingâ€ ï¼Œ Decoderï¼šâ€œResBlocks+upsamplingâ€
4. Channel Attentionï¼šJ. Hu, L. Shen, and G. Sun, â€œSqueeze-and-excitation networks,â€ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 7132â€“7141.ï¼ˆSEï¼‰
5. unpoolè¿™ä¸ªäºŒå€¼æ¨¡æ€æ©ç å°†å…¶ä»M X 1 X 1 -> M X 16 X 16 (åˆ†é…ç›¸åŒçš„å€¼ä»1X1åˆ°16X16), è¿™æ ·å­çš„è¯ï¼Œå°ºå¯¸ä¸ºM Ã— 16 Ã— 16çš„æœªåˆå¹¶çš„äºŒè¿›åˆ¶æ©æ¨¡å¯ä»¥ä¸å°ºå¯¸ä¸º128 Ã— 16 Ã— 16çš„Uå½¢å¤šæ¨¡æ€èåˆå—åœ¨é€šé“ç»´åº¦ä¸Šçš„ç“¶é¢ˆç‰¹å¾çº§è”ã€‚









#### Multi-Modal Joint Synthesisï¼šä¸ä»…åˆæˆç¼ºå¤±çš„æ¨¡æ€ï¼Œè€Œä¸”åˆæˆå·²æœ‰æ¨¡æ€

ç›¸æ¯”äºåˆ›æ–°ç‚¹æ¥è¯´å°±æ˜¯åŸæ¥éƒ½æ˜¯åªåˆæˆç¼ºå¤±çš„æ¨¡æ€ï¼Œè¿™é‡Œä¸¤ç§æƒ…å†µéƒ½åˆæˆ

å‰é¢ä¸€å¤§æ®µè¯å¤¸äº†ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹éå¸¸nice

ç„¶åè¯´æ˜åœ¨å‰å‘è¿‡ç¨‹ä¸­ï¼Œåªå¯¹ç¼ºå¤±çš„æ¨¡æ€è¿›è¡Œdiffusionï¼Œ*ï¼ˆæˆ‘ä¸æ˜ç™½è¿™é‡Œä¸ºä»€ä¹ˆåªå†™ä¸€ä¸ªdiffusionï¼Œåˆ°åº•æ˜¯åŠ å™ªè¿˜æ˜¯åšäº†ä»€ä¹ˆå…¶ä»–æ“ä½œï¼Œæ¨¡æ€ä¸æ˜¯ç¼ºå¤±äº†å—ï¼Œå›¾åƒå“ªé‡Œæ¥ï¼Ÿ**ä»¥ä¸‹æ˜¯è¯¢é—®DeepSeekçš„ç­”æ¡ˆ**ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€å¼€å§‹å›¾åƒéƒ½æ˜¯æœ‰çš„ï¼Œåªæ˜¯å¯¹éœ€è¦ç¼ºå¤±çš„æ¨¡æ€è¿›è¡Œäº†åŠ å™ªï¼Œç„¶åä¸éœ€è¦ç¼ºå¤±çš„æ¨¡æ€ä¸è¿›è¡ŒåŠ å™ªï¼‰*ï¼Œå¯ç”¨æ¨¡æ€ä¸åšå¤„ç†

![image-20251001204846999](imgs/med_seg/2-17.png)

![image-20251001205009760](imgs/med_seg/2-18.png)





åœ¨åå‘è¿‡ç¨‹ä¸­ï¼Œå¯ç”¨æ¨¡æ€ä¸¤ä¸ªç”¨é€”ï¼š1. ä½œä¸ºæ¡ä»¶ recover the **diffused** modalities ï¼ˆè¿™ä¸ªdiffusedåˆ°åº•æ˜¯è¡¨è¾¾ä»€ä¹ˆï¼Œç­”ï¼š**å·²ç»é€šè¿‡å‰å‘è¿‡ç¨‹è¢«å™ªå£°æ±¡æŸ“åçš„çŠ¶æ€**ï¼‰ 2. perform self-supervised representation learning 



**noteï¼š** the self-reconstructed available modalities are not used as input for the next denoising step.è‡ªæˆ‘é‡æ„çš„å¯ç”¨æ¨¡æ€å¹¶ä¸è¢«ç”¨ä½œä¸‹ä¸€ä¸ªå»å™ªæ­¥éª¤çš„è¾“å…¥ï¼Œä¹Ÿå°±æ˜¯è¯´æ¯ä¸€ä¸ªå»å™ªæ­¥éª¤çš„å¯ç”¨æ¨¡æ€éƒ½æ˜¯ä¸€å¼€å§‹å°±æä¾›å¥½çš„å›¾åƒäº†





#### Modality-Mask Scheme

ä½œç”¨ï¼š ä¸ºäº†ä½¿M2DNæ›´æœ‰æ•ˆåœ°é€‚åº”å„ç§æƒ…å†µä¸‹ä¸¢å¤±çš„æ¨¡æ€ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡æ€æ©ç æ–¹æ¡ˆï¼Œé€šè¿‡äºŒè¿›åˆ¶å‘é‡æ˜¾å¼ç¼–ç æ¨¡æ€å¯ç”¨æ€§çŠ¶æ€ã€‚

**In the training phase,** all the possible divisions between available and missing modalities are considered, which corresponds to all the reasonable possibilities of the binary vector, except all ones or all zeros casesã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œè€ƒè™‘å¯ç”¨æ¨¡æ€å’Œç¼ºå¤±æ¨¡æ€ä¹‹é—´çš„æ‰€æœ‰å¯èƒ½åˆ’åˆ†ï¼Œè¿™å¯¹åº”äºäºŒè¿›åˆ¶å‘é‡çš„æ‰€æœ‰åˆç†å¯èƒ½æ€§ï¼Œé™¤äº†å…¨1æˆ–å…¨0çš„æƒ…å†µï¼ˆè¿™æ®µè¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿéš¾é“ä¸åº”è¯¥æ˜¯æŠŠç¼ºå¤±çš„æ¨¡æ€åˆ’åˆ†ä¸º1ï¼Œå­˜åœ¨çš„æ¨¡æ€åˆ’åˆ†ä¸º0ï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆè€ƒè™‘æ‰€æœ‰çš„å¯èƒ½æ€§ï¼Ÿç­”ï¼šè¿™é‡Œå°±å¯¹åº”äºæˆ‘åˆšæ‰åœ¨Multi-Modal Joint Synthesisè¯´æ˜çš„æƒ…å†µäº†ï¼ˆä¸€å¼€å§‹å›¾åƒéƒ½æ˜¯æœ‰çš„ï¼Œåªæ˜¯å¯¹éœ€è¦ç¼ºå¤±çš„æ¨¡æ€è¿›è¡Œäº†åŠ å™ªï¼Œç„¶åä¸éœ€è¦ç¼ºå¤±çš„æ¨¡æ€ä¸è¿›è¡ŒåŠ å™ªï¼‰ï¼Œå› æ­¤è€ƒè™‘æ‰€æœ‰æƒ…å†µï¼Œ**åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šéšæœºåœ°ã€åå¤åœ°æ¨¡æ‹Ÿæ¯ä¸€ç§å¯èƒ½å‡ºç°çš„ç¼ºå¤±æƒ…å†µ**ï¼‰

![image-20251001213644253](imgs/med_seg/2-6.png)

![image-20251001213825046](imgs/med_seg/2-7.png)

![image-20251001213833636](imgs/med_seg/2-8.png)



**In the inference phase,** the binary vector is constructed according to input images, and serves as explicit guidance for the reverse diffusion process. åœ¨æ¨ç†é˜¶æ®µï¼Œæ ¹æ®è¾“å…¥å›¾åƒæ„é€ äºŒè¿›åˆ¶å‘é‡ï¼Œå¹¶ç”¨ä½œåå‘æ‰©æ•£è¿‡ç¨‹çš„æ˜¾å¼æŒ‡å¯¼ã€‚ï¼ˆæŠŠç¼ºå¤±çš„æ¨¡æ€åˆ’åˆ†ä¸º1ï¼Œå­˜åœ¨çš„æ¨¡æ€åˆ’åˆ†ä¸º0ï¼‰





#### æ¶ˆèå®éªŒ

![image-20251001203113665](imgs/med_seg/2-9.png)

U+J+Mï¼šå®Œæ•´æ¨¡å‹

U+J: å»æ‰ Modality-Mask Scheme

U+Mï¼šå»æ‰Multi-Modal Joint Synthesis

U:æ²¡æœ‰Modality-Mask Schemeä»¥åŠjoint synthesisï¼ˆè¿™ä¸ªå’ŒcDMçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿï¼‰ï¼Œè¿™ä¸ªæ˜¯å½“å‰çš„æ¨¡å‹ï¼Œä½†æ˜¯åªç”Ÿæˆç¼ºå¤±çš„æ¨¡æ€ä»¥åŠæ²¡æœ‰ä½¿ç”¨æ¨¡æ€æ©ç ç­–ç•¥

Jï¼šä»…ä»…Multi-Modal Joint Synthesis

cDMï¼šä¼ ç»Ÿçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä¸ºç‰¹å®šç¼ºå¤±åœºæ™¯å•ç‹¬è®­ç»ƒ





#### Loss Function

$$
L_{M2DN}=||\mathcal{M}-f_\theta(\mathcal{M}_t,t,v)||^2\\
væŒ‡çš„æ˜¯æ¨¡æ€æ©ç 
$$

since we add noise only to the missing modalities, not the observed ones in the diffusion process, performing self-reconstruction for observed modalities in the reverse diffusion step turns out to be more informative and effective than predicting zero noise for establishing common latent space. To maintain consensus between available and missing modalities, similarly, we estimate images, instead of noise, also for the missing modalities. ç”±äºæˆ‘ä»¬åªå¯¹ç¼ºå¤±æ¨¡æ€æ·»åŠ å™ªå£°ï¼Œè€Œä¸æ˜¯åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¯¹å¯ç”¨æ¨¡æ€æ·»åŠ å™ªå£°ï¼Œå› æ­¤åœ¨åå‘æ‰©æ•£æ­¥éª¤ä¸­å¯¹å¯ç”¨æ¨¡æ€æ‰§è¡Œè‡ªé‡å»ºæ¯”é¢„æµ‹é›¶å™ªå£°ä»¥å»ºç«‹å…¬å…±æ½œåœ¨ç©ºé—´æ›´æœ‰ä¿¡æ¯é‡å’Œæœ‰æ•ˆæ€§ï¼ˆè¿™æ®µè¯æ˜¯ä»€ä¹ˆæ„æ€ï¼ŒæŸå¤±å‡½æ•°åˆ°åº•æ˜¯é’ˆå¯¹å“ªéƒ¨åˆ†ï¼Ÿæ˜¯é’ˆå¯¹å»å™ªè¿‡ç¨‹ä¸­å¯¹åŠ å™ªçš„ç¼ºå¤±æ¨¡æ€è¿›è¡ŒæŸå¤±è®¡ç®—ï¼Œé‚£ä¹ˆæœªåŠ å™ªçš„å›¾åƒæ˜¯å¦‚ä½•è¿›è¡ŒæŸå¤±çš„ï¼Ÿï¼‰**ç»“è®ºæ˜¯ï¼š ä¸åŸå§‹çš„Diffusionçš„æŸå¤±ä¸åŒï¼Œè¿™éƒ¨åˆ†æ˜¯å¯¹å›¾åƒè¿›è¡ŒæŸå¤±ï¼Œå³æ¨¡å‹è¾“å‡ºçš„å›¾åƒä¸çœŸå®å›¾åƒè¿›è¡ŒæŸå¤±è®¡ç®—ï¼Œå¹¶ä¸å¯¹å™ªå£°è¿›è¡Œé¢„æµ‹å’ŒæŸå¤±è®¡ç®—**



#### æ•°æ®é›†

ä¸¤ä¸ªå…¬å…±å¤šæ¨¡æ€è„‘MRIæ•°æ®é›†ï¼šIXIï¼ˆT1w, T2w, and PD-weighted (PD).ï¼‰ å’Œ BraTS-2019 ï¼ˆT1w, T1wCE, T2w, and FLAIRï¼‰

We also apply the paired Wilcoxon signed-rank test with confidence interval p < 0.05 on the evaluation metric to statistically demonstrate the superiority of our M2DN compared to the other state-of-the-art methods. â€œæˆ‘ä»¬é‡‡ç”¨äº†**é…å¯¹Wilcoxonç¬¦å·ç§©æ£€éªŒ**è¿™ä¸€ç»Ÿè®¡æ–¹æ³•ï¼Œä»¥**æ¯ä¸ªæµ‹è¯•æ ·æœ¬ä¸ºå•ä½**ï¼Œä¸¥è°¨åœ°æ¯”è¾ƒäº†æˆ‘ä»¬çš„M2DNæ¨¡å‹ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•åœ¨å„ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚PSNRã€SSIMï¼‰ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å°†æ˜¾è‘—æ€§æ°´å¹³è®¾å®šä¸º `p < 0.05`ã€‚æ£€éªŒç»“æœè¯å®ï¼ŒM2DNçš„æ€§èƒ½æå‡åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¯**æ˜¾è‘—çš„**ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹ç¡®å®ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè€Œè¿™ç§ä¼˜åŠ¿æä¸å¯èƒ½æ˜¯ç”±å¶ç„¶å› ç´ é€ æˆçš„ã€‚â€ï¼ˆè¿™å®éªŒ666ï¼‰

![image-20251001211357934](imgs/med_seg/2-10.png)

![image-20251001212101446](imgs/med_seg/2-11.png)



![image-20251001211405275](imgs/med_seg/2-12.png)

![image-20251001212112796](imgs/med_seg/2-13.png)



**æ¨¡å‹å†…å­˜ä¸æ¨ç†æ—¶é—´**

![image-20251001211725211](imgs/med_seg/2-14.png)

**Tumor Segmentation**

å°†æ¨¡å‹ç”¨äºä¸‹æ¸¸ä»»åŠ¡ä¸­

![image-20251001214004972](imgs/med_seg/2-15.png)

![image-20251001214040615](imgs/med_seg/2-16.png)

























